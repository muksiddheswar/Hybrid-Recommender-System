{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TfIdfVectorizer from scikit-learn for text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Import CountVectorizer to create count matrix for tags\n",
    "# This is an alternative to tfidf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Requried to tokenise the text before Stemming\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "from porter2stemmer import Porter2Stemmer\n",
    "\n",
    "# Import linear_kernel for Cosine Similarity calculation of bodytext and title\n",
    "# This wil be applied on a tfidf matrix and NOT a count matrix\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the Cosine Similarity matrix based on a count_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# Funtions interacting with the database\n",
    "from db_functions import *\n",
    "\n",
    "# DB Queries generated in here\n",
    "from queries import *\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------#\n",
    "# MODEL CREATE HELPER FUNCTIONS\n",
    "#-------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "def filter_html(text):\n",
    "    soup = BeautifulSoup(text, features=\"html5lib\")\n",
    "    # text = re.sub('[^a-z\\s]', '',soup.get_text(separator=' ').lower())\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def text_stemmer (txt, stemmer):\n",
    "    token_words=word_tokenize(txt)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "\n",
    "\n",
    "def clean_tags(x):\n",
    "    if isinstance(x, str):\n",
    "        return str.lower(x.replace(\" \", \"\")).replace(\",\",\" \")\n",
    "\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------#\n",
    "# MODEL EXPORT HELPER FUNCTIONS\n",
    "#-------------------------------------#\n",
    "\n",
    "def matrix_to_jason(matrix):\n",
    "    df = pd.DataFrame(matrix.apply(lambda row: row.to_json(), axis=1), columns = ['jsol_col'])\n",
    "    df['local_id'] = df.index\n",
    "    return df\n",
    "\n",
    "\n",
    "def export_content_similarity (similarity_matrix):\n",
    "    df = matrix_to_jason(similarity_matrix)\n",
    "    sql = export_content_similarity_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "def export_title_similarity (similarity_matrix):\n",
    "    df = matrix_to_jason(similarity_matrix)\n",
    "    sql = export_title_similarity_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "def export_cat_tags_similarity (similarity_matrix):\n",
    "    df = matrix_to_jason(similarity_matrix)\n",
    "    sql = export_cat_tags_similarity_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "Model Created\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------#\n",
    "# MODEL CREATE DRIVER\n",
    "#-------------------------------------#\n",
    "\n",
    "\n",
    "truncate_similarities()\n",
    "article_master = import_content()\n",
    "\n",
    "\n",
    "\n",
    "## PREPROCESS CONTENT\n",
    "\n",
    "\n",
    "\n",
    "# REDUCE CONTENT:\n",
    "article_master['reduced_content'] = article_master.apply\\\n",
    "    (lambda row: re.sub('[^a-z\\s]', '',filter_html(row.bodytext).lower()), axis = 1)\n",
    "\n",
    "#-- Potential Global Variable\n",
    "\n",
    "# porter = PorterStemmer()\n",
    "snowball = Porter2Stemmer()\n",
    "\n",
    "article_master['stemmed_content'] = article_master.apply\\\n",
    "    (lambda row: text_stemmer(row.reduced_content, snowball), axis = 1)\n",
    "\n",
    "article_master['stemmed_content'] = article_master['stemmed_content'].fillna('')\n",
    "\n",
    "\n",
    "\n",
    "# REDUCE TITLE:\n",
    "# It must be noted that numbers are removed from the content and not from the title\n",
    "article_master['reduced_title'] = article_master.apply\\\n",
    "    (lambda row: re.sub('[^a-z0-9\\s]', '',row.title.lower()), axis = 1)\n",
    "\n",
    "article_master['stemmed_title'] = article_master.apply\\\n",
    "    (lambda row: text_stemmer(row.reduced_title, snowball), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# REDUCE TAGS AND CATEGORY\n",
    "article_master['reduced_category'] = article_master['category'].apply(clean_tags)\n",
    "article_master['reduced_tags'] = article_master['tags'].apply(clean_tags)\n",
    "article_master[\"meta_soup\"] = article_master[\"reduced_category\"] + ' ' + article_master['reduced_tags']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#-- At this point the newly stemmed metadata content can be written to the database.\n",
    "\"\"\"\n",
    "\n",
    "#-------------------------------------#\n",
    "## Preprocess Content - End\n",
    "#-------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# MODEL CREATION\n",
    "\n",
    "# Define a TF-IDF Vectorizer Object.\n",
    "# Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_content = tfidf.fit_transform(article_master['stemmed_content'])\n",
    "\n",
    "\n",
    "# Create additional step that uses TS-SS similarity.\n",
    "cosine_sim_content = linear_kernel(tfidf_matrix_content, tfidf_matrix_content)\n",
    "\n",
    "# Export content similarity matrix\n",
    "df = pd.DataFrame.from_records(cosine_sim_content)\n",
    "export_content_similarity(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfidf_matrix_title = tfidf.fit_transform(article_master['stemmed_title'])\n",
    "cosine_sim_title = linear_kernel(tfidf_matrix_title, tfidf_matrix_title)\n",
    "\n",
    "# Export title similarity matrix\n",
    "df = pd.DataFrame.from_records(cosine_sim_title)\n",
    "export_title_similarity(df)\n",
    "\n",
    "\n",
    "\n",
    "#-- Potential Global Variable\n",
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(article_master[\"meta_soup\"])\n",
    "cosine_sim_cat_tags = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "# Export title similarity matrix\n",
    "df = pd.DataFrame.from_records(cosine_sim_cat_tags)\n",
    "export_cat_tags_similarity(df)\n",
    "\n",
    "\n",
    "\n",
    "article_map = (article_master[['article_id','title']].copy()).drop_duplicates()\n",
    "article_map['local_id'] = article_map.index\n",
    "\n",
    "# Export article_map\n",
    "export_map(article_map)\n",
    "\n",
    "print(\"Model Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.10448185, 0.08291432, ..., 0.1221949 , 0.1737309 ,\n",
       "        0.25430651],\n",
       "       [0.10448185, 1.        , 0.2170747 , ..., 0.01331078, 0.02692974,\n",
       "        0.08882918],\n",
       "       [0.08291432, 0.2170747 , 1.        , ..., 0.01440918, 0.02898548,\n",
       "        0.07174855],\n",
       "       ...,\n",
       "       [0.1221949 , 0.01331078, 0.01440918, ..., 1.        , 0.08540134,\n",
       "        0.10624187],\n",
       "       [0.1737309 , 0.02692974, 0.02898548, ..., 0.08540134, 1.        ,\n",
       "        0.35348908],\n",
       "       [0.25430651, 0.08882918, 0.07174855, ..., 0.10624187, 0.35348908,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "trainVectorizerArray = vectorizer.fit_transform(article_master['stemmed_content'])\n",
    "transformer = TfidfTransformer()\n",
    "res = transformer.fit_transform(trainVectorizerArray)\n",
    "cos_sim = linear_kernel(res, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.10448185, 0.08291432, ..., 0.1221949 , 0.1737309 ,\n",
       "        0.25430651],\n",
       "       [0.10448185, 1.        , 0.2170747 , ..., 0.01331078, 0.02692974,\n",
       "        0.08882918],\n",
       "       [0.08291432, 0.2170747 , 1.        , ..., 0.01440918, 0.02898548,\n",
       "        0.07174855],\n",
       "       ...,\n",
       "       [0.1221949 , 0.01331078, 0.01440918, ..., 1.        , 0.08540134,\n",
       "        0.10624187],\n",
       "       [0.1737309 , 0.02692974, 0.02898548, ..., 0.08540134, 1.        ,\n",
       "        0.35348908],\n",
       "       [0.25430651, 0.08882918, 0.07174855, ..., 0.10624187, 0.35348908,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7600)\t0.0323988290846911\n",
      "  (0, 7593)\t0.03437920720840217\n",
      "  (0, 7554)\t0.044203027822719815\n",
      "  (0, 7448)\t0.03197450236689372\n",
      "  (0, 7426)\t0.08840605564543963\n",
      "  (0, 7361)\t0.018483561653999895\n",
      "  (0, 7351)\t0.0237767742494319\n",
      "  (0, 7346)\t0.0475535484988638\n",
      "  (0, 7345)\t0.09292158970487996\n",
      "  (0, 7336)\t0.036263849805273675\n",
      "  (0, 7332)\t0.04698271231670053\n",
      "  (0, 7275)\t0.02613203486239744\n",
      "  (0, 7223)\t0.044203027822719815\n",
      "  (0, 7216)\t0.023632767777440975\n",
      "  (0, 7134)\t0.05217460288604086\n",
      "  (0, 7132)\t0.0496429387552095\n",
      "  (0, 6932)\t0.07567748003436155\n",
      "  (0, 6897)\t0.030823938872783996\n",
      "  (0, 6886)\t0.0410208839199503\n",
      "  (0, 6836)\t0.025384027940294318\n",
      "  (0, 6808)\t0.03541735507889205\n",
      "  (0, 6758)\t0.05436001385597321\n",
      "  (0, 6753)\t0.030475075549150233\n",
      "  (0, 6739)\t0.027407504958946446\n",
      "  (0, 6727)\t0.0945369205760957\n",
      "  :\t:\n",
      "  (409, 6608)\t0.23385330101976834\n",
      "  (409, 6604)\t0.22098214315140088\n",
      "  (409, 6600)\t0.13638377581341257\n",
      "  (409, 5837)\t0.2830062151484228\n",
      "  (409, 5683)\t0.12589647480227914\n",
      "  (409, 5535)\t0.1633623696459554\n",
      "  (409, 5509)\t0.24201053348233292\n",
      "  (409, 5328)\t0.13235033364744445\n",
      "  (409, 5207)\t0.13554747276245946\n",
      "  (409, 5138)\t0.23385330101976834\n",
      "  (409, 4878)\t0.14573873323666106\n",
      "  (409, 4872)\t0.17572219725685592\n",
      "  (409, 4555)\t0.20673423325536688\n",
      "  (409, 4222)\t0.16184558335516752\n",
      "  (409, 4161)\t0.1315855131337341\n",
      "  (409, 3550)\t0.11909513127045708\n",
      "  (409, 3196)\t0.11631529881007611\n",
      "  (409, 3142)\t0.3266478940245844\n",
      "  (409, 2431)\t0.16037853682552583\n",
      "  (409, 2088)\t0.2124479468041792\n",
      "  (409, 2033)\t0.06825368050366155\n",
      "  (409, 1815)\t0.13392035124795357\n",
      "  (409, 1505)\t0.15368835089260297\n",
      "  (409, 781)\t0.24201053348233292\n",
      "  (409, 92)\t0.1562456703729914\n"
     ]
    }
   ],
   "source": [
    "print ((res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(stop_words='english', norm='None')\n",
    "res2 = tfidf.fit_transform(article_master['stemmed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3832)\t0.044203027822719815\n",
      "  (0, 3566)\t0.028931267090839943\n",
      "  (0, 3374)\t0.019179911664006385\n",
      "  (0, 3144)\t0.04964293875520949\n",
      "  (0, 7275)\t0.02613203486239744\n",
      "  (0, 1027)\t0.030823938872783993\n",
      "  (0, 2957)\t0.022443384092761094\n",
      "  (0, 741)\t0.03437117802332962\n",
      "  (0, 3785)\t0.01909941729144975\n",
      "  (0, 7448)\t0.03197450236689372\n",
      "  (0, 6171)\t0.03876311689023013\n",
      "  (0, 2681)\t0.03783874001718077\n",
      "  (0, 932)\t0.0248663810679727\n",
      "  (0, 5600)\t0.01770867753944602\n",
      "  (0, 4490)\t0.061647877745567986\n",
      "  (0, 867)\t0.018192856844951297\n",
      "  (0, 4868)\t0.029512876277474738\n",
      "  (0, 6753)\t0.03047507554915023\n",
      "  (0, 4922)\t0.04964293875520949\n",
      "  (0, 1364)\t0.029216685181921574\n",
      "  (0, 2862)\t0.036819739454714645\n",
      "  (0, 1195)\t0.04102088391995029\n",
      "  (0, 5265)\t0.04646079485243997\n",
      "  (0, 7351)\t0.023776774249431896\n",
      "  (0, 5433)\t0.04964293875520949\n",
      "  :\t:\n",
      "  (409, 4222)\t0.16184558335516752\n",
      "  (409, 5509)\t0.24201053348233292\n",
      "  (409, 6875)\t0.17000281581773208\n",
      "  (409, 1815)\t0.13392035124795357\n",
      "  (409, 2431)\t0.16037853682552583\n",
      "  (409, 4878)\t0.14573873323666106\n",
      "  (409, 1505)\t0.15368835089260297\n",
      "  (409, 7545)\t0.11369812525983401\n",
      "  (409, 5207)\t0.13554747276245946\n",
      "  (409, 92)\t0.1562456703729914\n",
      "  (409, 5535)\t0.1633623696459554\n",
      "  (409, 5328)\t0.13235033364744445\n",
      "  (409, 3196)\t0.11631529881007611\n",
      "  (409, 3550)\t0.11909513127045708\n",
      "  (409, 6600)\t0.13638377581341257\n",
      "  (409, 5683)\t0.12589647480227914\n",
      "  (409, 2033)\t0.06825368050366155\n",
      "  (409, 7361)\t0.10537173981490162\n",
      "  (409, 6677)\t0.19043915655501764\n",
      "  (409, 7134)\t0.14871940763278135\n",
      "  (409, 6727)\t0.05988215657854661\n",
      "  (409, 4161)\t0.1315855131337341\n",
      "  (409, 7346)\t0.13554747276245946\n",
      "  (409, 2088)\t0.2124479468041792\n",
      "  (409, 3142)\t0.3266478940245844\n"
     ]
    }
   ],
   "source": [
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<410x7666 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 54328 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Vectorizer to train set\n",
      "[[1 0 1 0]\n",
      " [0 1 0 1]]\n",
      "\n",
      "[[0.70710678 0.         0.70710678 0.        ]\n",
      " [0.         0.70710678 0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "train_set = [\"The sky is blue.\", \"The sun is bright.\"]  # Documents\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "trainVectorizerArray = vectorizer.fit_transform(train_set)\n",
    "trainVectorizerArray = trainVectorizerArray.toarray()\n",
    "\n",
    "print ('Fit Vectorizer to train set')\n",
    "print(trainVectorizerArray)\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "res = transformer.fit_transform(trainVectorizerArray)\n",
    "print()\n",
    "print ((res.todense()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70710678 0.         0.70710678 0.        ]\n",
      " [0.         0.70710678 0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "res1 = tfidf.fit_transform(train_set)\n",
    "print ((res1.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0.70710678 0.         0.70710678 0.        ]\n",
      " [0.         0.70710678 0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "tfidf1 = TfidfVectorizer(stop_words='english', norm='None')\n",
    "res2 = tfidf.fit_transform(train_set)\n",
    "print(type(res2))\n",
    "print ((res2.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
