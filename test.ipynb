{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TfIdfVectorizer from scikit-learn for text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Import CountVectorizer to create count matrix for tags\n",
    "# This is an alternative to tfidf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Requried to tokenise the text before Stemming\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "from porter2stemmer import Porter2Stemmer\n",
    "\n",
    "# Import linear_kernel for Cosine Similarity calculation of bodytext and title\n",
    "# This wil be applied on a tfidf matrix and NOT a count matrix\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the Cosine Similarity matrix based on a count_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# Funtions interacting with the database\n",
    "from db_functions import *\n",
    "\n",
    "# DB Queries generated in here\n",
    "from queries import *\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------#\n",
    "# MODEL CREATE HELPER FUNCTIONS\n",
    "#-------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "def filter_html(text):\n",
    "    soup = BeautifulSoup(text, features=\"html5lib\")\n",
    "    # text = re.sub('[^a-z\\s]', '',soup.get_text(separator=' ').lower())\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def text_stemmer (txt, stemmer):\n",
    "    token_words=word_tokenize(txt)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "\n",
    "\n",
    "def clean_tags(x):\n",
    "    if isinstance(x, str):\n",
    "        return str.lower(x.replace(\" \", \"\")).replace(\",\",\" \")\n",
    "\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------#\n",
    "# MODEL EXPORT HELPER FUNCTIONS\n",
    "#-------------------------------------#\n",
    "\n",
    "def matrix_to_jason(matrix):\n",
    "    df = pd.DataFrame(matrix.apply(lambda row: row.to_json(), axis=1), columns = ['jsol_col'])\n",
    "    df['local_id'] = df.index\n",
    "    return df\n",
    "\n",
    "\n",
    "def export_content_similarity (similarity_matrix):\n",
    "    df = matrix_to_jason(similarity_matrix)\n",
    "    sql = export_content_similarity_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "def export_title_similarity (similarity_matrix):\n",
    "    df = matrix_to_jason(similarity_matrix)\n",
    "    sql = export_title_similarity_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "def export_cat_tags_similarity (similarity_matrix):\n",
    "    df = matrix_to_jason(similarity_matrix)\n",
    "    sql = export_cat_tags_similarity_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "Model Created\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------#\n",
    "# MODEL CREATE DRIVER\n",
    "#-------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "article_master = import_content()\n",
    "\n",
    "\n",
    "\n",
    "## PREPROCESS CONTENT\n",
    "\n",
    "\n",
    "\n",
    "# REDUCE CONTENT:\n",
    "article_master['reduced_content'] = article_master.apply\\\n",
    "    (lambda row: re.sub('[^a-z\\s]', '',filter_html(row.bodytext).lower()), axis = 1)\n",
    "\n",
    "#-- Potential Global Variable\n",
    "\n",
    "# porter = PorterStemmer()\n",
    "snowball = Porter2Stemmer()\n",
    "\n",
    "article_master['stemmed_content'] = article_master.apply\\\n",
    "    (lambda row: text_stemmer(row.reduced_content, snowball), axis = 1)\n",
    "\n",
    "article_master['stemmed_content'] = article_master['stemmed_content'].fillna('')\n",
    "\n",
    "\n",
    "\n",
    "# REDUCE TITLE:\n",
    "# It must be noted that numbers are removed from the content and not from the title\n",
    "article_master['reduced_title'] = article_master.apply\\\n",
    "    (lambda row: re.sub('[^a-z0-9\\s]', '',row.title.lower()), axis = 1)\n",
    "\n",
    "article_master['stemmed_title'] = article_master.apply\\\n",
    "    (lambda row: text_stemmer(row.reduced_title, snowball), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# REDUCE TAGS AND CATEGORY\n",
    "article_master['reduced_category'] = article_master['category'].apply(clean_tags)\n",
    "article_master['reduced_tags'] = article_master['tags'].apply(clean_tags)\n",
    "article_master[\"meta_soup\"] = article_master[\"reduced_category\"] + ' ' + article_master['reduced_tags']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#-- At this point the newly stemmed metadata content can be written to the database.\n",
    "\"\"\"\n",
    "\n",
    "#-------------------------------------#\n",
    "## Preprocess Content - End\n",
    "#-------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# MODEL CREATION\n",
    "\n",
    "# Define a TF-IDF Vectorizer Object.\n",
    "# Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_content = tfidf.fit_transform(article_master['stemmed_content'])\n",
    "\n",
    "\n",
    "# Create additional step that uses TS-SS similarity.\n",
    "cosine_sim_content = linear_kernel(tfidf_matrix_content, tfidf_matrix_content)\n",
    "\n",
    "# Export content similarity matrix\n",
    "df = pd.DataFrame.from_records(cosine_sim_content)\n",
    "export_content_similarity(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfidf_matrix_title = tfidf.fit_transform(article_master['stemmed_title'])\n",
    "cosine_sim_title = linear_kernel(tfidf_matrix_title, tfidf_matrix_title)\n",
    "\n",
    "# Export title similarity matrix\n",
    "df = pd.DataFrame.from_records(cosine_sim_title)\n",
    "export_title_similarity(df)\n",
    "\n",
    "\n",
    "\n",
    "#-- Potential Global Variable\n",
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(article_master[\"meta_soup\"])\n",
    "cosine_sim_cat_tags = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "# Export title similarity matrix\n",
    "df = pd.DataFrame.from_records(cosine_sim_cat_tags)\n",
    "export_cat_tags_similarity(df)\n",
    "\n",
    "\n",
    "\n",
    "article_map = (article_master[['article_id','title']].copy()).drop_duplicates()\n",
    "article_map['local_id'] = article_map.index\n",
    "\n",
    "# Export article_map\n",
    "export_map(article_map)\n",
    "\n",
    "print(\"Model Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'matrix' object has no attribute 'todense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3d043c6af3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_matrix_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_matrix_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtfidf_matrix_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'matrix' object has no attribute 'todense'"
     ]
    }
   ],
   "source": [
    "tfidf_matrix_content = tfidf_matrix_content.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_content = linear_kernel(tfidf_matrix_content, tfidf_matrix_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cosine_sim_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Vectorizer to train set\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0.70710678 0.         0.70710678 0.        ]\n",
      " [0.         0.70710678 0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "train_set = [\"The sky is blue.\", \"The sun is bright.\"]  # Documents\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "trainVectorizerArray = vectorizer.fit_transform(train_set)\n",
    "# print(type(trainVectorizerArray))\n",
    "\n",
    "# trainVectorizerArray = trainVectorizerArray.toarray()\n",
    "\n",
    "print ('Fit Vectorizer to train set')\n",
    "# print(trainVectorizerArray)\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "res = transformer.fit_transform(trainVectorizerArray)\n",
    "print(type(res))\n",
    "print ((res.todense()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0.70710678 0.         0.70710678 0.        ]\n",
      " [0.         0.70710678 0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "res1 = tfidf.fit_transform(train_set)\n",
    "print(type(res1))\n",
    "print ((res1.todense()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
