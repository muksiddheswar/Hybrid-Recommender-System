{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL connection is closed\n",
      "MySQL connection is closed\n",
      "Previous Model Truncated.\n",
      "Pre-processing....\n",
      "Creating new Model.\n",
      "MySQL connection is closed\n",
      "Exported Content Cosine Similarity Matrix .\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 20 18:00:07 2019\n",
    "\n",
    "@author: smkj33\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Previous Version: \n",
    "    bkp_model_20_Aug.py\n",
    "\n",
    "-- Removing html filter using HTML.parser \n",
    "-- Removing related import statement\n",
    "-- Beautiful soup will be used in it's place'\n",
    "\n",
    "\n",
    "    bkp_model_19_Oct.py \n",
    "    \n",
    "--  Replaced Potter Stemmer with Snowball Stemmer\n",
    "--  Added Cosine similarity with TS-SS.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "from porter2stemmer import Porter2Stemmer\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "\n",
    "from db_functions import *\n",
    "from queries import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------#\n",
    "# MODEL CREATE HELPER FUNCTIONS\n",
    "#-------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "def filter_html(text):\n",
    "    soup = BeautifulSoup(text, features=\"html5lib\")\n",
    "    # text = re.sub('[^a-z\\s]', '',soup.get_text(separator=' ').lower())\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_stemmer (txt, stemmer):\n",
    "    token_words=word_tokenize(txt)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "\n",
    "def clean_tags(x):\n",
    "    if isinstance(x, str):\n",
    "        return str.lower(x.replace(\" \", \"\")).replace(\",\",\" \")\n",
    "\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def theta(cosine_similarity):\n",
    "    sim = np.divide(np.trunc(np.multiply(cosine_similarity, 100000000000000)), 100000000000000)\n",
    "    angles = np.arccos(sim) + math.radians(10)\n",
    "    return angles\n",
    "\n",
    "\n",
    "def magnitude_diff(matrix):\n",
    "    magnitude = np.sqrt(matrix.multiply(matrix).sum(1))\n",
    "    magnitude_diff = pairwise_distances(magnitude, metric='manhattan')\n",
    "    return magnitude_diff\n",
    "\n",
    "\n",
    "def euclidean(vectors):\n",
    "    distances = euclidean_distances(vectors)\n",
    "    return distances\n",
    "\n",
    "\n",
    "#-------------------------------------#\n",
    "# MODEL EXPORT HELPER FUNCTIONS\n",
    "#-------------------------------------#\n",
    "\n",
    "def matrix_to_json(matrix):\n",
    "    df = pd.DataFrame(matrix.apply(lambda row: row.to_json(), axis=1), columns = ['data_col'])\n",
    "    df['local_id'] = df.index\n",
    "    return df\n",
    "\n",
    "\n",
    "def export_content_cosine_similarity (similarity_matrix):\n",
    "    df = matrix_to_json(similarity_matrix)\n",
    "    sql = export_content_cosine_similarity_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "def export_title_similarity (similarity_matrix):\n",
    "    df = matrix_to_json(similarity_matrix)\n",
    "    sql = export_title_similarity_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "def export_cat_tags_similarity (similarity_matrix):\n",
    "    df = matrix_to_json(similarity_matrix)\n",
    "    sql = export_cat_tags_similarity_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "def export_content_angles(angles):\n",
    "    df = matrix_to_json(angles)\n",
    "    sql = export_content_angles_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "def export_content_distance(distance):\n",
    "    df = matrix_to_json(distance)\n",
    "    sql = export_content_distance_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "def export_content_magnitude(vector_size):\n",
    "    df = matrix_to_json(vector_size)\n",
    "    df['local_id'] = df.index\n",
    "    sql = export_content_magnitude_query()\n",
    "    export_data(df, sql)\n",
    "\n",
    "\n",
    "#-------------------------------------#\n",
    "# MODEL CREATE DRIVER\n",
    "#-------------------------------------#\n",
    "\n",
    "\n",
    "truncate_similarities()\n",
    "article_master = import_content()\n",
    "\n",
    "\n",
    "\n",
    "## PREPROCESS CONTENT\n",
    "print(\"Previous Model Truncated.\")\n",
    "print(\"Pre-processing....\")\n",
    "\n",
    "\n",
    "# REDUCE CONTENT:\n",
    "article_master['reduced_content'] = article_master.apply\\\n",
    "    (lambda row: re.sub('[^a-z\\s]', '',filter_html(row.bodytext).lower()), axis = 1)\n",
    "\n",
    "#-- Potential Global Variable\n",
    "\n",
    "snowball = Porter2Stemmer()\n",
    "\n",
    "article_master['stemmed_content'] = article_master.apply\\\n",
    "    (lambda row: text_stemmer(row.reduced_content, snowball), axis = 1)\n",
    "\n",
    "article_master['stemmed_content'] = article_master['stemmed_content'].fillna('')\n",
    "\n",
    "\n",
    "\n",
    "# REDUCE TITLE:\n",
    "# It must be noted that numbers are removed from the content and not from the title\n",
    "article_master['reduced_title'] = article_master.apply\\\n",
    "    (lambda row: re.sub('[^a-z0-9\\s]', '',row.title.lower()), axis = 1)\n",
    "\n",
    "article_master['stemmed_title'] = article_master.apply\\\n",
    "    (lambda row: text_stemmer(row.reduced_title, snowball), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# REDUCE TAGS AND CATEGORY\n",
    "article_master['reduced_category'] = article_master['category'].apply(clean_tags)\n",
    "article_master['reduced_tags'] = article_master['tags'].apply(clean_tags)\n",
    "article_master[\"meta_soup\"] = article_master[\"reduced_category\"] + ' ' + article_master['reduced_tags']\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------#\n",
    "## Preprocess Content - End\n",
    "#-------------------------------------#\n",
    "\n",
    "print(\"Creating new Model.\")\n",
    "\n",
    "# MODEL CREATION\n",
    "\n",
    "# Define a TF-IDF Vectorizer Object for Un-normalised TF-IDF vectors\n",
    "# Remove all english stop words such as 'the', 'a'\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', norm = None)\n",
    "tfidf_vectors = tfidf.fit_transform(article_master['stemmed_content'])\n",
    "\n",
    "\n",
    "cosine_sim_content = cosine_similarity(tfidf_vectors)\n",
    "\n",
    "# Export content similarity matrix\n",
    "df = pd.DataFrame.from_records(cosine_sim_content)\n",
    "export_content_cosine_similarity(df)\n",
    "print(\"Exported Content Cosine Similarity Matrix .\")\n",
    "\n",
    "# Theta, Euclidean Distance and Magnitude of TF-IDF vectors: required for TS-SS similarity\n",
    "angles = theta(cosine_sim_content)\n",
    "euclidean_distance = euclidean(tfidf_vectors)\n",
    "magnitude_diff = magnitude_diff(tfidf_vectors)\n",
    "ed_md_square = np.square(euclidean_distance + magnitude_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.        , 39071.10905259, 40957.62025929, ...,\n",
       "        52971.23486319, 45801.36905505, 52306.92063022],\n",
       "       [39071.10905259,     0.        ,  7537.34256237, ...,\n",
       "        13241.90521519, 11060.82090836, 13129.77169996],\n",
       "       [40957.62025929,  7537.34256237,     0.        , ...,\n",
       "        10874.3531324 ,  8994.32241858, 10848.3396339 ],\n",
       "       ...,\n",
       "       [52971.23486319, 13241.90521519, 10874.3531324 , ...,\n",
       "            0.        ,  3742.86799106,  1067.60868544],\n",
       "       [45801.36905505, 11060.82090836,  8994.32241858, ...,\n",
       "         3742.86799106,     0.        ,  3167.88902328],\n",
       "       [52306.92063022, 13129.77169996, 10848.3396339 , ...,\n",
       "         1067.60868544,  3167.88902328,     0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_md_square"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
